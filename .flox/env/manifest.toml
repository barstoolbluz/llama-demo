## Flox Environment Manifest -----------------------------------------
##
##   _Everything_ you need to know about the _manifest_ is here:
##
##   https://flox.dev/docs/reference/command-reference/manifest.toml/
##
## -------------------------------------------------------------------
# Flox manifest version managed by Flox CLI
version = 1


## Install Packages --------------------------------------------------
##  $ flox install gum  <- puts a package in [install] section below
##  $ flox search gum   <- search for a package
##  $ flox show gum     <- show all versions of a package
## -------------------------------------------------------------------
[install]
python311.pkg-path = "python311"
pip.pkg-path = "python311Packages.pip"
huggingface-hub.pkg-path = "python311Packages.huggingface-hub"
git.pkg-path = "git"
# gum.pkg-path = "gum"
# gum.version = "^0.14.5"


## Environment Variables ---------------------------------------------
##  ... available for use in the activated environment
##      as well as [hook], [profile] scripts and [services] below.
## -------------------------------------------------------------------
[vars]
# HF_TOKEN was used for initial model download - no longer needed


## Activation Hook ---------------------------------------------------
##  ... run by _bash_ shell when you run 'flox activate'.
## -------------------------------------------------------------------
[hook]
# on-activate = '''
#   # -> Set variables, create files and directories
#   # -> Perform initialization steps, e.g. create a python venv
#   # -> Useful environment variables:
#   #      - FLOX_ENV_PROJECT=/home/user/example
#   #      - FLOX_ENV=/home/user/example/.flox/run
#   #      - FLOX_ENV_CACHE=/home/user/example/.flox/cache
# '''


## Profile script ----------------------------------------------------
## ... sourced by _your shell_ when you run 'flox activate'.
## -------------------------------------------------------------------
[profile]
# common = '''
#   gum style \
#   --foreground 212 --border-foreground 212 --border double \
#   --align center --width 50 --margin "1 2" --padding "2 4" \
#     $INTRO_MESSAGE
# '''
## Shell-specific customizations such as setting aliases go here:
# bash = ...
# zsh  = ...
# fish = ...


## Services ---------------------------------------------------------
##  $ flox services start             <- Starts all services
##  $ flox services status            <- Status of running services
##  $ flox activate --start-services  <- Activates & starts all
## ------------------------------------------------------------------
[services]
# myservice.command = "python3 -m http.server"


## Include ----------------------------------------------------------
## ... environments to create a composed environment
## ------------------------------------------------------------------
[include]
# environments = [
#     { dir = "../common" }
# ]


## Build and publish your own packages ------------------------------
##  $ flox build
##  $ flox publish
## ------------------------------------------------------------------
[build]

[build.llama-model]
description = "LLaMA 2 7B HuggingFace model"
version = "2.0.0"
command = """
#!/usr/bin/env bash
set -euo pipefail

echo "Packaging LLaMA 2 7B model (using symlinks)..."

# Create output directory structure
mkdir -p "$out/share/models"

# Get absolute path to model files
MODEL_SRC="$(cd models/llama-2-7b-hf && pwd)"

# Create symlink to model directory
ln -s "$MODEL_SRC" "$out/share/models/llama-2-7b-hf"

# Verify
if [ -f "$out/share/models/llama-2-7b-hf/config.json" ]; then
  echo "✓ Model packaged successfully (symlinked)"
  echo "  Target: $MODEL_SRC"
  echo "  Link: $out/share/models/llama-2-7b-hf"
else
  echo "✗ Model packaging failed"
  exit 1
fi
"""

[build.llama-server]
description = "LLaMA 2 7B Inference Server (FastAPI)"
version = "1.0.0"
command = """
#!/usr/bin/env bash
set -euo pipefail

echo "Building LLaMA 2 Inference Server..."

# Create virtualenv in $out
python -m venv "$out"
source "$out/bin/activate"

# Install dependencies
echo "Installing dependencies..."
pip install --upgrade pip setuptools wheel
pip install fastapi uvicorn transformers torch --extra-index-url https://download.pytorch.org/whl/cpu

# Copy server script
echo "Installing server application..."
mkdir -p "$out/lib/llama-server"
cp server/server.py "$out/lib/llama-server/"

# Create launcher script
cat > "$out/bin/llama-server" <<'EOF'
#!/usr/bin/env bash
set -euo pipefail

# Activate virtualenv
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/activate"

# Set default MODEL_PATH if not provided
if [ -z "${MODEL_PATH:-}" ]; then
    echo "ERROR: MODEL_PATH environment variable not set"
    echo "Usage: MODEL_PATH=/path/to/model llama-server [OPTIONS]"
    exit 1
fi

# Run server
cd "$(dirname "$SCRIPT_DIR")/lib/llama-server"
exec uvicorn server:app --host 0.0.0.0 --port 8000 "$@"
EOF

chmod +x "$out/bin/llama-server"

echo ""
echo "✓ LLaMA Server built successfully!"
echo ""
echo "Usage:"
echo "  MODEL_PATH=/path/to/model llama-server"
echo ""
"""


## Other Environment Options -----------------------------------------
[options]
# Systems that environment is compatible with
# systems = [
#   "aarch64-darwin",
#   "aarch64-linux",
#   "x86_64-darwin",
#   "x86_64-linux",
# ]
# Uncomment to disable CUDA detection.
# cuda-detection = false
