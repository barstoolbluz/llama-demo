apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-inference
  labels:
    app: llama-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llama-inference
  template:
    metadata:
      labels:
        app: llama-inference
      annotations:
        # Reference to Flox environment on FloxHub
        # Update 'owner' to your FloxHub username/org
        flox.dev/environment: "owner/llama-runtime"
        # Optional: Pin to specific generation for production
        # flox.dev/environment: "owner/llama-runtime@12"
        # Optional: Disable metrics
        # flox.dev/disable-metrics: "true"
    spec:
      runtimeClassName: flox
      containers:
        - name: server
          # Stub image required by Flox runtime (49 bytes)
          image: flox/empty:1.0.0
          # Command runs inside Flox environment activation
          command: ["llama-server"]
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          env:
            # MODEL_PATH is set by Flox environment
            # Can override here if needed:
            # - name: MODEL_PATH
            #   value: "/custom/path"
            - name: SERVER_HOST
              value: "0.0.0.0"
            - name: SERVER_PORT
              value: "8000"
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          resources:
            requests:
              memory: "8Gi"
              cpu: "2000m"
            limits:
              memory: "16Gi"
              cpu: "4000m"
